# CityLearn-RL-Project ğŸ‡ªğŸ‡¸ğŸ™ï¸âš¡ï¸

**GestiÃ³n energÃ©tica inteligente de edificios con Deep Reinforcement Learning**

Proyecto personal para demostrar (y aprender) cÃ³mo diseÃ±ar, entrenar y evaluar agentes de RL capaces de reducir consumo, coste y picos de demanda manteniendo el confort tÃ©rmico en un distrito virtual de edificios (ğŸ› ï¸ **CityLearn**).

---

## âœ¨ Objetivos

1.  **Minimizar energÃ­a total** consumida por edificio.
2.  **Minimizar la factura** elÃ©ctrica aplicando tarifas horarias dinÃ¡micas.
3.  **Peak-shaving** â†’ reducir picos mÃ¡ximos de potencia.
4.  **Balancear confort vs. consumo** penalizando salidas del rango 21â€“24 Â°C.
5.  Explorar **baterÃ­as y fotovoltaica** (carga/descarga Ã³ptima).
6.  Comparar **mÃºltiples agentes** (DQN, PPO, SAC, reglas, random).
7.  Visualizar resultados en un **dashboard interactivo** (Streamlit).

*Cada objetivo serÃ¡ un experimento reproducible con mÃ©trica(s) clara(s).*

---

## ğŸ“ Estructura

```
CityLearn-RL-Project/
â”‚
â”œâ”€â”€ README.md               â† este archivo
â”œâ”€â”€ requirements.txt        â† dependencias Python
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ data/                   â† escenarios CityLearn (climas, configsâ€¦)
â”‚
â”œâ”€â”€ notebooks/              â† exploraciÃ³n, demo y anÃ¡lisis visual
â”‚   â”œâ”€â”€ 00_exploracion_datos.ipynb
â”‚   â”œâ”€â”€ 01_baselines_reglas.ipynb
â”‚   â””â”€â”€ 02_experimentos_RL.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py             â† lanza entrenamiento segÃºn config YAML
â”‚   â”œâ”€â”€ train.py            â† loop de entrenamiento RL
â”‚   â”œâ”€â”€ evaluate.py         â† evalÃºa polÃ­ticas y guarda mÃ©tricas
â”‚   â”œâ”€â”€ agents/             â† DQN, PPO, SAC, Random, ReglaFijaâ€¦
â”‚   â”œâ”€â”€ env/                â† wrappers CityLearn + funciones reward
â”‚   â”œâ”€â”€ utils/              â† mÃ©tricas, visualizaciÃ³n, parsing config
â”‚   â””â”€â”€ monitoring/         â† panel Streamlit (en vivo / post-hoc)
â”‚
â”œâ”€â”€ configs/                â† *.yaml con parÃ¡metros de experimento
â”‚
â”œâ”€â”€ results/                â† logs, checkpoints, grÃ¡ficos, CSV mÃ©tricas
â””â”€â”€ tests/                  â† tests unitarios (PyTest)
```

---

## ğŸš€ InstalaciÃ³n rÃ¡pida

```bash
# 1. Clona el repo
git clone [https://github.com/tu-usuario/CityLearn-RL-Project.git](https://github.com/tu-usuario/CityLearn-RL-Project.git)
cd CityLearn-RL-Project

# 2. Crea entorno virtual
python -m venv .venv
source .venv/bin/activate          # Windows: .venv\Scripts\activate

# 3. Instala dependencias
pip install -r requirements.txt

# 4. Prueba un entorno CityLearn
python - <<'PY'
from citylearn import CityLearn
env = CityLearn(data_path='data/Climate_Zone_1/')
print('Estado inicial OK:', env.reset()[:5])
PY
```

### ğŸƒâ€â™‚ï¸ CÃ³mo lanzar un experimento

```bash
python src/main.py --config configs/exp_min_energia.yaml
```
`main.py` lee el YAML: selecciona agente, reward, horizonte temporal y semilla; guarda todo en `results/`.

### ğŸ“Š Monitor en vivo

```bash
streamlit run src/monitoring/dashboard.py
```
VerÃ¡s:
* Curva de reward por episodio
* EvoluciÃ³n de temperatura interior/exterior
* Consumo vs. baseline
* Picos de potencia, coste acumulado, COâ‚‚, etc.

---

## ğŸ“ˆ MÃ©tricas clave

| MÃ©trica                | DescripciÃ³n                             |
| :--------------------- | :-------------------------------------- |
| **`energy_kWh`** | EnergÃ­a total consumida                 |
| **`cost_eur`** | Factura (â‚¬) con tarifa TOU              |
| **`peak_kw`** | Potencia mÃ¡xima horaria                 |
| **`comfort_violations_h`** | Horas fuera de 21â€“24 Â°C                 |
| **`co2_kg`** | Emisiones (si el escenario lo permite)  |

*Todas se guardan en `results/metrics/experiment_id.csv`.*

---

## ğŸ““ Notebooks recomendados
* `00_exploracion_datos` â€“ quÃ© variables hay en CityLearn y cÃ³mo se comportan.
* `01_baselines_reglas` â€“ regla de control sencilla y agente aleatorio.
* `02_experimentos_RL` â€“ carga checkpoints, compara agentes y genera plots listos para papers.

---

## ğŸ› ï¸ AÃ±adir un nuevo objetivo / reward
1.  Implementa tu funciÃ³n en `src/env/reward_functions.py`.
2.  AÃ±ade la entrada `reward_type:` en un YAML de `configs/`.
3.  Lanza `main.py` con la nueva config.

*La mÃ©trica aparecerÃ¡ automÃ¡ticamente en el dashboard y en los CSV.*

---

## ğŸ§ª Tests

```bash
pytest -q
```
Nos aseguramos de que:
* El wrapper del entorno devuelva estados con el `shape` correcto.
* Las mÃ©tricas calculen bien ahorro y confort.
* Los agentes produzcan acciones dentro de los lÃ­mites.

---

## ğŸ“œ CrÃ©ditos y licencias
* **CityLearn** Â© Intelligent Environments Lab - MIT License
* **Este proyecto**: Apache 2.0 (ver LICENSE).

---

## ğŸ¤ Contacto
JosÃ© ArbelÃ¡ez (UAM) â€“ jose.ancizar.667@gmail.com

Â¡Feliz optimizaciÃ³n energÃ©tica! âš¡ï¸ğŸ¢